\documentclass{book}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{listings}
\usepackage[colorlinks]{hyperref}
\usepackage[dvipsnames]{xcolor}

\hypersetup{
    colorlinks=true,
    linkcolor=black,
    filecolor=magenta,      
    urlcolor=teal,
}

\lstset{
    language=R,
    numbers=left,
    rulecolor=\color{black},
    numberstyle=\tiny\color{Gray},
    commentstyle=\color{Gray},
    stringstyle=\color{MidnightBlue},
    keywordstyle=\color{black},
    numberstyle=\tiny\color{Gray},
    literate={~} {$\sim$}{1}
}

\title{The zen of programming}
\author{Emilio Berti}
\date{\today}

\begin{document}

\maketitle

\tableofcontents

\chapter{Introduction}

I have been told that my programming skills are above average. Often, I am asked to develop or debug code and to explain why I coded scripts in the way I did. I realized that I follow a combination of personal rules-of-thumbs, rules-of-thumbs of other people who code better than me, and style guides from notorious people (e.g. \href{https://style.tidyverse.org/}{Wickham tidyverse guide style}) or big companies (e.g. \href{https://google.github.io/styleguide/Rguide.html}{Googleâ€™s R Style Guide}); they are doing better than me, and copying their style is probably a good idea. 

In this guide, I want to show some tips and tricks I follow routinely and to explain why I do some things in a certain way. I am sure better and more comprehensive guides already exist, but maybe not for people with ecological background, which is often fragmentary regarding coding.

\section{General tips}

These are my \textit{laws} that I always try to follow. In some cases I violate some of them, e.g. in early-development or testing dozens of statistical models without clue of the underlying data. However, a final, releasable code should always follow all these laws. If I will release a code that does not follow one of these laws, I will be ashamed of myself -- except in the case I want to prove a point on them. It is important to stress that these laws apply specifically to the scientific research environment and are not representative of how to properly code in other settings. The laws are:

\begin{enumerate}
    \item Data input, manipulation, and output must be explicit.
    \item Do not overflow the (global) environment. Really guys, we are ecologists, be nice to the environment.
    \item Do not nest more than three loops/conditional statements. If you did, rewrite everything from scratch.
    \item If you're gonna do it twice, write a function for it.
\end{enumerate}

These laws are fancy and general enough to be mis-understood. Let's expand on them.

\subsection{How (not) to work with files}

Nowadays, most of the analyses require large computations divided into steps. Intermediate output can be stored into files, which then can be used for downstream computations. A common mistake is to work with these intermediate files using point-and-click methods, often copy-pasting their content into scripts. This is an extremely bad practice for many reasons. First, there is no trace of what it has been done and from where the text in the script is coming from. Second, text editors often use special characters, e.g. linebreaks, that are not compatible within a script or among operating systems. Thirds, the potential for automation is severely reduced; for intsance, if you work with hundreds of files, the point-an-click steps need to be re-performed manually every time. Finally, the code is less readable, especially in the case the \textit{csv} contains many rows. 

An easy way to avoid all of this is to read the file in the code using reading functions, e.g. \texttt{read.csv()}, \texttt{read\_csv()}, or \texttt{fread()} in \textit{R}. This may seem basic, but it happens more than what I would like to admit.

\subsection{Global environment overflow}

I have seen many times a phenomenon that I call environment overflow, i.e. the (re)initialization of variables contained in a dataframe without deleting old copies. For example, a column (\textit{x}) in a dataframe (\textit{df}) can be extracted and passed to a new variable: \lstinline[columns=fixed]{x <- df$x}. I consider this a bad practice because: first, it does not provide any new information; second, it created duplicates in the environment; and finally, it creates confusion for everyone (for instance, what's the difference between \textit{x} and \textit{d\$x}?).

I have seen this used particularly when performing statistical tests or modelling. All main functions related to these tasks accept a \textit{data} argument, i.e. the dataframe where the variables are stored. So, why not to use directly this argument and avoid overflowing the working environment? For instance, it is preferable to write

\begin{lstlisting}[showstringspaces=false]
m <- lm(y ~ x, data = df)
\end{lstlisting}

instead of 

\begin{lstlisting}[showstringspaces=false]
x <- df$x
y <- df$y
m <- lm(y ~ x)
\end{lstlisting}

which takes more space and is less clear (where are \textit{x} and \textit{y} coming from?), especially when \texttt{lm()} is called many lines after the first two or when there are multiple dataframes with the same variable names (is it \textit{x} from \textit{df1} or from \textit{df2}?). Despite not overflowing directly the environment, also the \texttt{attach()} function in \textit{R} generates similar confusions and should thus be avoided.

\subsection{Nesting for/if chunks}
Let's take a look at the following code comparing values from three vectors. Values are compared and then the relationships between them are reported.

\begin{lstlisting}[showstringspaces=false]
x <- rnorm(100) #100 random normally distributed values
y <- rnorm(100)
z <- rnorm(100)
ans <- rep(NA, 100) #initialize answer
for (i in seq_along(x)) {
   if (x[i] > 0) {
      if (y[i] < z[i]) {
         if (y[i] < x[i]) {
            ans[i] <- "x > 0, x > y, y < z"
         } else {
            ans[i] <- "x > 0, x < y, y < z"
         }
      } else {
         ans[i] <- "x > 0, y > z"
      }
   } else {
      ans[i] <- "x < 0"
   }
}

ans[1:10]

 [1] "x < 0"               "x < 0"               "x > 0, y > z"       
 [4] "x < 0"               "x > 0, x > y, y < z" "x < 0"              
 [7] "x < 0"               "x < 0"               "x < 0"              
[10] "x > 0, y > z"   
\end{lstlisting}

The code above runs ok, performs the task it needs to do, but it can barely be read and understood. I can assure you that this is because there are four nested for/if statements. If you remove them, not only the code will be much readable, but, at least in \textit{R}, it will also run faster. Let's try to rewrite it:

\begin{lstlisting}[showstringspaces=false]
ans[x > 0 & x > y & y < z] <- "x > 0, x > y, y < z"
ans[x > 0 & x < y & y < z] <- "x > 0, x < y, y < z"
ans[x > 0 & y > z] <- "x > 0, y > z"
ans[x < 0] <- "x < 0"

ans[1:10]

 [1] "x < 0"               "x < 0"               "x > 0, y > z"       
 [4] "x < 0"               "x > 0, x > y, y < z" "x < 0"              
 [7] "x < 0"               "x < 0"               "x < 0"              
[10] "x > 0, y > z"   
\end{lstlisting}

It sure isn't pretty and it can still be improved, but just by removing the nested statements and using \textit{R} native vectorized operator \texttt{\&} we achieve the same task using four instead of 15 messy, unreadable lines. Also, remember that in \textit{R} vectorized operations are always the preferred native way of doing things, whereas for/if chuncks are quite slow and unefficient; we hit two birds with the same stone here.

\subsection{Use functions for transferable, manageable code}

Functions are you're biggest friends when you need to re-do the same tasks multiple times. In \textit{R} functions are declated as:

\begin{lstlisting}[showstringspaces=false]
my_fun <- function(arg1, arg2, ...) {
   # something to compute
   # . . . 
   # something to return
}
\end{lstlisting}

where \textit{my\_fun} is the name of your function and \textit{arg1} and \textit{arg2} the arguments of the function. A simple function is the power of a number:

\begin{lstlisting}[showstringspaces=false]
squared <- function(x) { #x is the number you want the power of
  ans <- x ** 2 #compute
  return(ans) #return
}

squared(2)

[1] 4
\end{lstlisting}

This function is quite useless, but it is useful to play with such useless functions to get a grasp on them. A more complex function can be to get the power of a number with random exponent between one and 10:

\begin{lstlisting}[showstringspaces=false]
# compute the power *n* of a number,
# with *n* being randomly sampled between 1 and 10.
random_squared <- function(x) {
  root <- runif(1, 0.1, 1) * 10
  root <- round(root)
  ans <- x ** root
  message("The random exponent is: ", root)
  return(ans)
}

random_squared(1:5)

The random exponent is: 5
[1]    1   32  243 1024 3125
\end{lstlisting}

In \textit{R} it is not necessary to return something and \texttt{return(x)} is the same as \texttt{x}. I learnt coding in \textit{C}, where returns must be specified, and I prefer to explicitly write it. I couldn't find a negative consequence of explicitly returning the output, so I do it because it is more clear what it is returned.

Just to give an idea of how useful functions can be, let's take a look at one I have used:

\begin{lstlisting}[showstringspaces=false]
#' @title get correct UTM crs for the study area
#' @param df data.frame with "lon", "lat" coordinates.
#' @return crs in format "CRS" (sp package).
utm_crs <- function(df) {
  if (!"lon" %in% colnames(df) | !"lat" %in% colnames(df)) {
    stop("Missing 'lon' or 'lat' column")
  }
  lon <- df[, "lon"]
  range_lon <- range(lon)
  avg_lon <- mean(range_lon)
  lat <- df[, "lat"]
  range_lat <- range(lat)
  avg_lat <- mean(range_lat)
  utm <- floor((avg_lon + 180) / 6) + 1
  epsg <- 32600 + utm
  if (avg_lat < 0) {
    epsg <- epsg + 100
  }
  ans <- raster::crs(paste0("EPSG:", epsg))
  return(ans)
}

\end{lstlisting}

I did this because I wanted to obtain a UTM coordinate reference system from a lon-lat degree one. It is something that you can write down every time you need it, but by declaring the function I can call it where needed, without the need to copy-paste wildly. Also, if there is a mistake in the function (e.g. I should add 120 instead of 100 at line 17), I need to change this only once instead of several times in several scripts, with the risk that I forget to change it in all occurences, leading to error in the code.

\chapter{R}

\section{Readable code}
If a code runs, good. If a code that runs is readable, great. Rarely, a good, functioning code is written at the first attempt. Often, code written some time before need to be changed. If code is not readable, changes are difficult to implement. Therefore the question: how can we write readable code? 

There is a lot of emphasis in academia on learning how to write scientific papers for journals, but not on how to write proper code. To researchers, I suggest to write code as they would write a manuscript for a scientific paper. Divide the whole code into manageable stand-alone scripts that fit a purpose, e.g. data preparation (Introduction), analysis (Results), and visualization (Discussion). Divide each script in chunks as you would do with paragraphs, e.g. a first chunk to load the data, another one to explore it and apply necessary transformations, another one to save the transformed data into a new file. Treat each script and chunk as you would do with a manuscript section and paragraph: if a chunk is very long, split it (make a new paragraph); if a script is too long, split it into two (make a new section); if a chunk is not necessary for the main analysis, make a stand-alone script for it (move it to the appendix).

Some principles that I came up specifically for \textit{R}:

\begin{enumerate}
    \item Never let RStudio to save your workspace as \textit{.RData}. If you want to save a \textit{.RData} or \textit{rds} data, save it explicitly. No data should be saved without users explicitly asking for it.
\end{enumerate}

\chapter{python}

\chapter{Bash}
\lstset{language=bash}

Bash is a Unix shell that provides command line user interface to the GNU/Linux operating system. Bash is one of the main reasons I prefer Linux over Windows. It comes with a pre-defined set of commands useful for job control and file and directory utilities. For instance, Bash \texttt{find} makes it easy to locate files in the whole hard drive. The command to find a file containing the string \textit{LICENSE} in its name is:

\begin{lstlisting}
$ find . -maxdepth 2 -name '*LICENSE*'

./django-polls/LICENSE
./keras/LICENSE
./freetube/LICENSES.chromium.html
./freetube/LICENSE.electron.txt
./julia-1.5.2/LICENSE.md
./Downloads/LICENSES.chromium.html
./Downloads/LICENSE.electron.txt
\end{lstlisting}

the option \texttt{-maxdepth 2} limits the search within two children directories of the current location.

A comprehensive list of all useful commands is not in the scope of this guide, but the ones I use most often are:

\begin{itemize}
   \item[] \texttt{echo} prints strings on the terminal screen
   \item[] \texttt{cd} changes directory
   \item[] \texttt{pwd} prints the absolute path of the current directory
   \item[] \texttt{mkdir} creates a directory
   \item[] \texttt{touch} creates a file
   \item[] \texttt{nano} starts the \textit{nano} text editor in the terminal 
   \item[] \texttt{rm} removes files or directories
   \item[] \texttt{ls} lists contents of the current directory
   \item[] \texttt{grep} shows only files or strings containing a specific pattern
   \item[] \texttt{cp} copies an existing files or directory to a new location
   \item[] \texttt{mv} moves an existing files or directory to a new location
   \item[] \texttt{tree} shows the directory tree of the current location
   \item[] \texttt{ssh} connects via secure shell to remote machines
   \item[] \texttt{history} shows the last commands run in Bash
   \item[] \texttt{cat} prints out a single file or concatenate several ones
   \item[] \texttt{head} prints the first lines of a file
   \item[] \texttt{tail} prints the last lines of a file
   \item[] \texttt{more} prints a file in the terminal with navigation control
   \item[] \texttt{tr} removes or subtitute characters in a string or a file
   \item[] \texttt{cut} separates a string or file according to a character and retrieve only specific columns
   \item[] \texttt{chmod} administrates reading, writing, and executing priviledges of files
   \item[] \texttt{ps} shows running processes
   \item[] \texttt{kill} terminates processes
   \item[] \texttt{git} for git version control
   \item[] \texttt{zip/unzip} zips or unzips files
   \item[] \texttt{wget} downloads stuff from internet
   \item[] \texttt{curl} downloads stuff from internet
   \item[] \texttt{man} shows the manual of a command
\end{itemize}

Pressing \texttt{Ctrl + r} starts a reverse search of the recently-used commands.

Command can be piped using \texttt{|}, where the output returned by the left expression is used as input by the right expression. For example, \lstinline[columns=fixed]{ls | grep *.pdf} will show only the files in the current directory that have \textit{pdf} extension. This can be used to perform tasks that otherwise will require manual labour in a straightforward way. For intance, it happends quite often that we have multiple \textit{csv} files that we want to concatenate (bind them row-wise) into one file. This can be done in other programming languages as \textit{R} or \textit{python}, but it is much easier (and faster) to do it in Bash:

\begin{lstlisting}
find . -maxdepth 1  -name '*.csv' -print0 | xargs -0 cat > onefile.csv
\end{lstlisting}

The operator \texttt{>} redirect the output to \textit{onefile.csv}, where the content of all csv files will be stored. At this point you may have noticed that the above code, when \texttt{-maxdepth 1} changes to other numers, will not only concatenate files within the current directory, but also in all children directories depending on the number specified. This is an example of an extremely tedious task that is made extremely easy in Bash. The code above may look complicated, but once you get used to Bash it comes naturally to your mind, much before thinking of an alternative solution in \textit{R} (you will need to use at least the three functions \texttt{list.files()}, \texttt{read.csv()}, and \texttt{rbind()}).

It may not be clear from this simple list why Bash is so powerful or what can be achieved by using it. But it is indeed the best companion to perform automated pipelines in a secure and scalable way. Bash is substantially an environment where it is possible to code in a programming language that is useful to perform operations on files or strings and to control processes and their flow. You can also add custom functionality specifying aliases (more about this below) and functions, most notably in the \textit{~/.bashrc} file that is sourced when a Bash terminal is open. As an example consider the following function that I added to the \textit{~/.bashrc} file:

\begin{lstlisting}
uppercase() {
   echo $1 | tr  '[:lower:]' '[:upper:]'
   echo $1 | tr  '[:lower:]' '[:upper:]' | xclip -sel clip
}

$ uppercase 'hello world!'

HELLO WORLD!
\end{lstlisting}

In Bash, \textit{\$n} (where \textit{n} is a number) means that that is an argument passed to the function. In the above code, the \texttt{uppercase()} function prints the passed argument (a string) and pass it (using \texttt{|}) to \texttt{tr} to replace lowercase characters with uppercase ones. The third line does the same thing but copies the output one the clipboard, so I can paste it using \textit{Ctrl + v}.

\section{aliases}
Aliases renames existing command (or pipes of them) in one word that you find more familiar. For instance, I can never rememebr, so I added this to the \textit{~/.bashrc}:

\begin{lstlisting}
alias clip="xclip -selection c"
\end{lstlisting}

Instead of writing \lstinline[columns=fixed]{xclip -selection c}, I can now only write \lstinline[columns=fixed]{clip}, which will implicitly performs the same thing.\\

\section{Bash scripts}

You can also write scripts that can be called in Bash. As an example, below is a script I wrote to get general information about the food additives \textit{E\#\#\#} from wikipedia:

\lstinputlisting{wiki.sh}

I saved the script in a file called \textit{wiki.sh} file, which can be called in bash running \lstinline[columns=fixed]{$ bash wiki.sh} or by giving it running priviledges:

\begin{lstlisting}
$ chmod +x wiki.sh
$ ./wiki.sh E150
E150, Caramel color , , https://en.wikipedia.org/wiki/E150
$ ./wiki.sh E214
E214, Ethylparaben , antifungal, https://en.wikipedia.org/wiki/E214
\end{lstlisting}

The line \lstinline[columns=fixed]{#!/bin/bash} tells Bash which program to use to run the script, in this case Bash itself. If you want to run a \textit{python} script \textit{script.py} directly from Bash you can either do it by \lstinline[columns=fixed]{$ python script.py} or by substituting \lstinline[columns=fixed]{#!/bin/bash} with \lstinline[columns=fixed]{#!/bin/python}, giving it running priviledges and run it with \lstinline[columns=fixed]{$ ./script.py}.

Linux has a native way to schedule jobs, e.g. running a script. One of the most popular are \textit{cron} and \textit{crontab}. I'll not give further details here, but \textit{crontab} provides an easy interface to schedule periodic jobs, e.g. uploading data to a remote location, running check-ups and updates, etc.

\chapter{Web servers}

\section{Apache2}

Apache2 is a popular web server software to host a website. To install apache2 in Ubuntu:

\begin{lstlisting}
$ sudo apt update
$ sudo apt install apache2
\end{lstlisting}

Sites are stored in the \textit{/var/www} folder.

\begin{lstlisting}
$ sudo mkdir /var/www/gci
$ sudo nano /var/www/gci/index.html #write your html site
$ cd /etc/apache2/sites-available/
$ sudo cp 000-default.conf gci.conf
$ sudo nano gci.conf
####### in nano add ###################
# ServerAdmin emilio.berti90@gmail.com
# DocumentRoot /var/www/gci/
# ServerName gci.example.com
#######################################
$ sudo a2ensite gci.conf
$ service apache2 reload
# sudo nano /etc/hosts
####### in nano add ###########
# 127.0.1.1    gci.example.com
####### in nano add ###########
\end{lstlisting}

\chapter{Spatial analysis}

The most basic ideas of spatial analysis are \textbf{rasters} and \textbf{geometries}. A raster is a basically a matrix to which it has been associated several meta-data, defining for instance its geographic projection, resolution, etc. For most purposes, you should think to a raster as a pixel image with ancillary data to map it into a real place on Earth. Rasters can be stacked, similarly to multi-dimensional matrices (tensors); in R, these are called \textit{RasterStack} (or bricks, I never understood the difference). To be stackable, rasters need to have the same geographic projection, resolution, and extent. Each raster in a stack is called a \textit{layer} (sometimes also feature, especially in machine learning). Geometries are vector representations of spatial features. For example, a line is represented by a vector with origin, end, and module and are thus ``scale-free''. Rasters and geometries were developed for a common goal (mapping spatial features), but contrained by two different phylosophies, i.e. the necessity to map pixel-like features or continuous ones. If you are familiar with PDF file format, you should know that PDF encode, when possible, all information as vector geometries and, when it cannot, as pixel rasters. That's why you can zoom indefinitely in some PDFs and they still maintain the original resolution, but in others the image becomes blurry; you just hit the resolution limit of that pixel raster.

\section{R packages}
The world of spatial analysis is huge and scary. There are many packages to work with spatial stuff. In R, the most notable are \textbf{raster} (to be superseeded by the much faster \textbf{terra}) for raster analysis and \textbf{sp} for vector analysis. You need to install \textbf{gdal} (abstraction library) and \textbf{geos} (geometric representation) to use them (and \textbf{rgdal} and \textbf{rgeos} as well). Make sure you install all relevant libraries and packages, as some coordiante reference systems (more about this below) are contained only in one of them.
I personally like also the \textbf{sf} package, a tidyverse substitute for \textit{sp}. It is a more high-level interface to spatial geometries and it is thus easier to use than \textit{sp}; it works great in conjuction with \textit{dplyr} and, if you're using tidyverse, I strongly recommend to use \textit{sf}. For low-level stuff, \textit{sp} is probably more flexible and perhaps faster.

\section{python packages}
Will write this, for now: \textbf{shapely}, \textbf{fiona}, \textbf{GeoJSON}, and \textbf{GeoPandas}. Python is great to build pipelines, websites and visualizations. I cannot stress how much flexible and scalable python is in this regard.

\section{Coordinate Reference System (CRS)}
Rasters and geometries need a CRS to properly map their features in space. 
CRS tells us how to interpet pixels and vector in a precise spatial context.
You can think to a CRS as ticks of xy axes of a scatterplot; without the ticks, we can see the points, but we have no information about how to interpet them. 
A CRS also defines the measurement unit, with the most commonly used being arc-degrees and meters.

CRS are commonly expressed as the \textit{EPSG} code and by their \textit{proj4string} expression. For most purposes, EPSG codes and proj4string are equivalent; EPSG (which stands for European Petroleum Survey Group) codes are IDs that refer to a specific proj4string. I find it easier to remember a four/five digit EPSG code than a complex proj4string, but that's me. Proj4string is, however, more explicit in its definition. A proj4string typically has many parameters, stored together as a string. For example:

\begin{itemize}
    \item[+] init, an EPSG code (e.g. \texttt{+init=epsg:4326})
    \item[+] proj, the projection used (e.g. \texttt{+proj=merc})
    \item[+] ellps, the ellipsoid model of Earth (e.g. \texttt{+ellps=WGS84})
    \item[+] units, the measurement units (e.g. \texttt{+units=m})
\end{itemize}

A list of all parameters can be found at \href{https://proj.org/operations/projections/tpers.html?highlight=lat\_0#parameters}{https://proj.org/} It is important to understand that all EPSG codes and proj4strings are community standards, i.e. definition of spatial coordinates that are useful for specific purposes. Nothing forbids you to define a custom proj4string, but, if it is really useful, it is probably already in the GDAL archive.

The website \href{https://epsg.io/}{https://epsg.io/} contains all information you need to know about a particular CRS and how to convert it to another one. 
In R, you can get a list of all installed CRS with \textit{rgdal::make\_EPSG()}, which returns a data.frame with, among the others, the EPSG code and the proj4string of projection. 
As this is a data.frame, you can also search for a specific CRS. For instance, if you want only projections that include the string \textit{berlin}, you can achieve it with the following code:

\begin{lstlisting}
library(rgdal)
crs <- make_EPSG() #this has 6609 rows for me
berlin <- crs[grep('berlin', crs$note, ignore.case = TRUE),
              c("code", "prj4")] #only one for me
\end{lstlisting}
\textit{berlin} is the CRS called Soldener Berlin \textit{EPSG:3068}, with proj4 \texttt{+proj=cass +lat\_0=52.4186482777778 +lon\_0=13.6272036666667 +x\_0=40000 +y\_0=10000 +ellps=bessel +units=m +no\_defs +type=crs}: \href{https://epsg.io/3068}{https://epsg.io/3068}. 
From the proj4string is clear that the unit of measurment is meters, a Bessel ellipsoid id used to approximate Earth surface, and the origin is at a longitude of 13.63 and latitude of 52.42. 
I was not aware of what a Bessel ellipsoid was, but a quick search susggests this is used for national surveys (which makes sense here for Germany) and that it will be replaced in the next decades by modern ellipsoids of satellite geodesy. 
This hints to a good point: standards change, so keep in mind that analyses made years ago may need to be adjusted for today's standards.

\section{Spatial analysis in R 101}
\subsection{Reproject rasters and geoemtries}

\end{document}
